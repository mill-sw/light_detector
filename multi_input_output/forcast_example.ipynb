{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# read the dataset into python\n",
    "df = pd.read_csv('household_power_consumption.txt', delimiter=';')\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['date_time'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "df['Global_active_power'] = pd.to_numeric(df['Global_active_power'], errors='coerce')\n",
    "df = df.dropna(subset=['Global_active_power'])\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "df = df.loc[:, ['date_time', 'Global_active_power']]\n",
    "df.sort_values('date_time', inplace=True, ascending=True)\n",
    "df = df.reset_index(drop=True)\n",
    "print('Number of rows and columns after removing missing values:', df.shape)\n",
    "print('The time series starts from: ', df['date_time'].min())\n",
    "print('The time series ends on: ', df['date_time'].max())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.info()\n",
    "df.head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Split into training, validation and test datasets.\n",
    "# Since it's timeseries we should do it by date.\n",
    "test_cutoff_date = df['date_time'].max() - timedelta(days=7)\n",
    "val_cutoff_date = test_cutoff_date - timedelta(days=14)\n",
    "df_test = df[df['date_time'] > test_cutoff_date]\n",
    "df_val = df[(df['date_time'] > val_cutoff_date) & (df['date_time'] <= test_cutoff_date)]\n",
    "df_train = df[df['date_time'] <= val_cutoff_date]\n",
    "\n",
    "#check out the datasets\n",
    "print('Test dates: {} to {}'.format(df_test['date_time'].min(), df_test['date_time'].max\n",
    "print('Validation dates: {} to {}'.format(df_val['date_time'].min(), df_val['date_time'].max\n",
    "print('Train dates: {} to {}'.format(df_train['date_time'].min(), df_train['date_time'].max"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Goal of the model:\n",
    "# Predict Global_active_power at a specified time in the future.\n",
    "# Eg. We want to predict how much Global_active_power will be ten minutes from now.\n",
    "# We can use all the values from t-1, t-2, t-3, .... t-history_length to predict t\n",
    "def create_ts_files(dataset, start_index, end_index, history_length, step_size, target_step, num_rows_per_file, data_folder):\n",
    "    assert step_size > 0\n",
    "    assert start_index >= 0\n",
    "\n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "\n",
    "    time_lags = sorted(range(target_step + 1, target_step + history_length + 1, step_size), re\n",
    "    col_names = [f'x_lag{i}' for i in time_lags] + ['y']\n",
    "    start_index = start_index + history_length\n",
    "    if end index is None:\n",
    "        end_index - len(dataset) - target_step\n",
    "    rng = range(start_index, end_index)\n",
    "    num_rows = len(rng)\n",
    "    num_files = math.ceil(num_rows / num_rows_per_file)\n",
    "\n",
    "    # for each file.\n",
    "    print(f'Creating {num_files} files.')\n",
    "    for i in range(num_files):\n",
    "        filename = f'{data_folder}/ts_file{i}.pkl'\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'{filename}')\n",
    "\n",
    "        # get the start and end indices.\n",
    "        ind0 = i * num_rows_per_file\n",
    "        ind1 = min(ind0 + num_rows_per_file, end_index)\n",
    "        data_list = []\n",
    "\n",
    "        # j in the current timestep. Will need j-n to j-1 for the history. And j + targe\n",
    "        for j in range(ind0, ind1):\n",
    "            indices = range(j - 1, j - history_length - 1, -step_size)\n",
    "            data = dataset[sorted(indices) + [j + target_step]]\n",
    "\n",
    "            # append data to the list.\n",
    "            data_list.append(data)\n",
    "        df_ts = pd.DataFrame(data=data_list, columns=col_names)\n",
    "        df_ts.to_pickle(filename)\n",
    "    \n",
    "    return len(col_names)-1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "global_active_power = df_train['Global_active_power'].values\n",
    "# Scaled to work with Neural networks.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "global_active_power_scaled = scaler.fit_transform(global_active_power.reshape(-1, 1)).re\n",
    "history_length = 7*24*60 # The history length in minutes.\n",
    "step_size = 10 # The sampling rate of the history. Eg. If step_size = 1, then values fr\n",
    " # If step size = 10 then values ev\n",
    "target_step = 10 # The time step in the future to predict. Eg. If target_step = 0, then\n",
    " # If target_step = 10 then\n",
    "# The csv creation returns the number of rows and number of features. We need these valu\n",
    "num_timesteps = create_ts_files(global_active_power_scaled, start_index=0,end_index=None,history_length=history_length,\n",
    "step_size=step_size,target_step=target_step,num_rows_per_file=128*100, data_folder='ts_data')\n",
    "# I found that the easiest way to do time series with tensorflow is by creating pandas f\n",
    "# the value to predict y = x{t+n}. We tried doing it using TFRecords, but that API is no\n",
    "# The resulting file using these parameters is over 17GB. If history_length is increased\n",
    "# Hard to fit into laptop memory, so need to use other means to load the data from the h"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# So we can handle loading the data in chunks from the hard drive instead of having to l\n",
    "\n",
    "# The reason we want to do this is so we can do custom processing on the data that we ar\n",
    "# LSTM requires a certain shape and it is tricky to get it right.\n",
    "#\n",
    "class TimeSeriesLoader:\n",
    "d f i i lf f ld fil f\n",
    "Get started Open in app\n",
    "time_series_loader.py hosted with ❤ by GitHub view raw\n",
    " def __init__(self, ts_folder, filename_format):\n",
    " self.ts_folder = ts_folder\n",
    "\n",
    " # find the number of files.\n",
    " i = 0\n",
    " file_found = True\n",
    " while file_found:\n",
    " filename = self.ts_folder + '/' + filename_format.format(i)\n",
    " file_found = os.path.exists(filename)\n",
    " if file_found:\n",
    " i += 1\n",
    "\n",
    " self.num_files = i\n",
    " self.files_indices = np.arange(self.num_files)\n",
    " self.shuffle_chunks()\n",
    "\n",
    " def num_chunks(self):\n",
    " return self.num_files\n",
    "\n",
    " def get_chunk(self, idx):\n",
    " assert (idx >= 0) and (idx < self.num_files)\n",
    "\n",
    " ind = self.files_indices[idx]\n",
    " filename = self.ts_folder + '/' + filename_format.format(ind)\n",
    " df_ts = pd.read_pickle(filename)\n",
    " num_records = len(df_ts.index)\n",
    "\n",
    " features = df_ts.drop('y', axis=1).values\n",
    " target = df_ts['y'].values\n",
    "\n",
    " # reshape for input into LSTM. Batch major format.\n",
    " features_batchmajor = np.array(features).reshape(num_records, -1, 1)\n",
    " return features_batchmajor, target\n",
    "\n",
    " # this shuffles the order the chunks will be outputted from get_chunk.\n",
    " def shuffle_chunks(self):\n",
    " np.random.shuffle(self.files_indices)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}