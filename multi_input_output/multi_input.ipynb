{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "colab": {
   "name": "20201110_combined_model_fixed.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "accelerator": "GPU",
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# --Index--\n",
    "## 1. Setup\n",
    "## 2. Model\n",
    "## 3. Data Process\n",
    "## 4. Results\n",
    "```\n",
    "   4-1 gel\n",
    "   4-2 cap\n",
    "   4-3 gel+att\n",
    "   4-4 cap+att\n",
    "   4-5 gel+cap+att\n",
    "   +(CM, ROC, AUC)\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 Setup"
   ],
   "metadata": {
    "id": "Uq4RV-rbvw4S"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import datetime, cv2, csv, os, glob, locale\n",
    "from pprint import pprint\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import *\n",
    "# from keras.layers.core import Activation, Dropout, Dense\n",
    "# from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as k\n",
    "from tensorflow.math import confusion_matrix\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 텐서보드 활성화\n",
    "%load_ext tensorboard"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-28 09:29:05.340427: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "metadata": {
    "id": "hR4ro952hTmN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def create_mlp(dim, regress=False):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "\tmodel.add(Dense(4, activation=\"relu\"))\n",
    "\n",
    "\t# check to see if the regression node should be added\n",
    "\tif regress:\n",
    "\t\tmodel.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "\treturn model\n",
    "\n",
    "def create_cnn(width, height, depth, filters=(16, 32, 64), regress=False):\n",
    "\t# initialize the input shape and channel dimension, assuming TensorFlow/channels-last ordering\n",
    "\tinputShape = (height, width, depth)\n",
    "\tchanDim = -1\n",
    "\tinputs = Input(shape=inputShape)\n",
    "\n",
    "\t# loop over the number of filters\n",
    "\tfor (i, f) in enumerate(filters):\n",
    "\t\t# if this is the first CONV layer then set the input appropriately\n",
    "\t\tif i == 0:\n",
    "\t\t\tx = inputs\n",
    "\n",
    "\t\t# CONV => RELU => BN => POOL\n",
    "\t\tx = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "\t# flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "\tx = Flatten()(x)\n",
    "\tx = Dense(16)(x)\n",
    "\tx = Activation(\"relu\")(x)\n",
    "\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\tx = Dropout(0.5)(x)\n",
    "\n",
    "\t# apply another FC layer, this one to match the number of nodes coming out of the MLP\n",
    "\tx = Dense(4)(x)\n",
    "\tx = Activation(\"relu\")(x)\n",
    "\n",
    "\t# check to see if the regression node should be added\n",
    "\tif regress:\n",
    "\t\tx = Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "\tmodel = Model(inputs, x)\n",
    "\n",
    "\treturn model"
   ],
   "outputs": [],
   "metadata": {
    "id": "yvyQFtVoR7XC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def draw_CM(label, predicted):\n",
    "    cm = confusion_matrix(label, predicted)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    # true : false rate\n",
    "    true = 0\n",
    "    false = 0\n",
    "    for i, j in enumerate(label):\n",
    "        if j != predicted[i] :\n",
    "            false += 1\n",
    "        else: true += 1\n",
    "\n",
    "    classification_report = metrics.classification_report(label, predicted)\n",
    "\n",
    "    multilabel_to_binary_matrics = metrics.multilabel_confusion_matrix(label, predicted)\n",
    "\n",
    "    return plt.show(), print('true rate: ', true), print('false rate: ', false), print(), print('='*10, 'classification_report: ', '\\n', classification_report), print('='*10, 'multilabel_to_binary_matrics by class_num: ','\\n','[[TN / FP] [FN / TP]]','\\n', multilabel_to_binary_matrics)\n",
    "def draw_ROC_AUC(x, y, category_names):\n",
    "    n_classes = len(category_names)\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y[:, i], x[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y.ravel(), x.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "            label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:0.2f})', \n",
    "            color='deeppink', linestyle=':', linewidth=1)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "            label=f'macro-average ROC curve (area = {roc_auc[\"macro\"]:0.2f})',\n",
    "            color='navy', linestyle=':', linewidth=1)\n",
    "\n",
    "    colors = (['purple', 'pink', 'red', 'green', 'yellow', 'cyan', 'magenta', 'blue'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=1, label=f'Class {i} ROC curve (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "    plt.xlim([-.01, 1.0])\n",
    "    plt.ylim([0.0, 1.01])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC & AUC')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    return plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Data Preprocessing\n",
    "\n",
    "파일 이름안에 정보가 다 들어 있다.\n",
    "\n",
    "예를 들어 SEP_APR/090122113_180113_81_6.2_3.5_0_ca.png 파일의 경우\n",
    "```\n",
    "SEP_APR/090122113_180113_81_6.2_3.5_0_ca.png\n",
    "    APR : Dx_1. 분류하려는 카테고리이름\n",
    "               81 :Age    6.2 :Protein_serum    3.5 :Albumin    0 :Previous_Dx\n",
    "```"
   ],
   "metadata": {
    "id": "nwVolPeqxYwQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "########## Make DataFrame ##########\n",
    "DATA_PATH = '201027_EPAI_tot_modi'\n",
    "file_path = os.path.join(DATA_PATH)\n",
    "category_names = os.listdir(DATA_PATH)\n",
    "classes = list(range(len(category_names)))\n",
    "\n",
    "data = []\n",
    "gel_files=[]\n",
    "cap_files=[]\n",
    "\n",
    "for i in glob.glob(file_path + \"**/**/*.png\", recursive=True):\n",
    "    dirname = os.path.dirname(i)\n",
    "    dir_basename = os.path.basename(dirname)\n",
    "    for num, name in enumerate(category_names):\n",
    "        if dir_basename == name:\n",
    "            y = num\n",
    "\n",
    "    basename = os.path.basename(i)\n",
    "    basename = basename.replace('.png', '')\n",
    "    info = basename.split(\"_\")\n",
    "\n",
    "    if len(info) < 6:\n",
    "        continue\n",
    "    if 'ca' in info[5]:\n",
    "        continue\n",
    "\n",
    "    att = info[2], info[3], info[4], info[5]\n",
    "    att = list(map(float, att))\n",
    "\n",
    "    img = 'cap' if len(info) == 6 else 'gel'\n",
    "\n",
    "    if len(data) < 3:\n",
    "        data.append([i, y, att, img])\n",
    "    else:\n",
    "        if data[-1][3] == img:\n",
    "            data.pop()\n",
    "            data.append([i, y, att, img])\n",
    "        else:\n",
    "            data.append([i, y, att, img])\n",
    "\n",
    "for i, j in enumerate(data):\n",
    "    gel_files.append(j) if data[i][3] == 'gel' else cap_files.append(j)\n",
    "\n",
    "df = pd.DataFrame(data, columns=['path', 'y', 'att', 'img'])\n",
    "df\n",
    "\n",
    "df.to_csv('data.csv')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '201027_EPAI_tot_modi'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10408/382598876.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mDATA_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'201027_EPAI_tot_modi'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcategory_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '201027_EPAI_tot_modi'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 이미지\n",
    "gel = []\n",
    "gel1 = []\n",
    "cap = []\n",
    "# 속성\n",
    "att = []\n",
    "# 카테고리 index\n",
    "y = []\n",
    "\n",
    "GEL_WIDTH = 256\n",
    "GEL_HEIGHT = 160\n",
    "CAP_WIDTH = 128\n",
    "CAP_HEIGHT = 32\n",
    "\n",
    "for gel_file in gel_files:\n",
    "    gel_img = cv2.imread(gel_file[0], cv2.IMREAD_GRAYSCALE)\n",
    "    gel_resize = cv2.resize(gel_img, dsize=(GEL_WIDTH, GEL_HEIGHT), interpolation=cv2.INTER_AREA)\n",
    "    gel.append(gel_resize)\n",
    "\n",
    "    gel_resize1 = cv2.resize(gel_img, dsize=(CAP_WIDTH, CAP_HEIGHT), interpolation=cv2.INTER_AREA)\n",
    "    gel1.append(gel_resize1)\n",
    "\n",
    "    att.append(gel_file[2])\n",
    "    y.append(gel_file[1])\n",
    "\n",
    "for cap_file in cap_files:\n",
    "    cap_img = cv2.imread(cap_file[0], cv2.IMREAD_GRAYSCALE)\n",
    "    cap_resize = cv2.resize(cap_img, dsize=(CAP_WIDTH, CAP_HEIGHT), interpolation=cv2.INTER_AREA)\n",
    "    cap.append(cap_resize)\n",
    "\n",
    "gel = np.array(gel)\n",
    "gel = gel / 255.0\n",
    "gel1 = np.array(gel1)\n",
    "gel1 = gel1 / 255.0\n",
    "cap = np.array(cap)\n",
    "cap = cap / 255.0\n",
    "att = np.array(att)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"gel.shape :\", gel.shape)\n",
    "print(\"cap.shape :\", cap.shape)\n",
    "print(\"attr.shape :\", att.shape)\n",
    "print(\"y.shape :\", y.shape)\n",
    "print(\"gel1.shape :\", gel1.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Standardize -> Shuffle -> Split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# att Standardize\n",
    "for i in range(len(att[0])):\n",
    "  target_column = att[:,i]\n",
    "  mean = np.mean(target_column)\n",
    "  centered = target_column - mean\n",
    "  norms = np.linalg.norm(centered)\n",
    "  normed = centered / norms\n",
    "  att[:,i] = normed.T\n",
    "\n",
    "# Shuffle\n",
    "# shuffled_index = np.random.RandomState(seed=1).permutation(len(gel))\n",
    "shuffled_index = np.random.permutation(len(gel))\n",
    "gel = gel[shuffled_index]\n",
    "cap = cap[shuffled_index]\n",
    "att = att[shuffled_index]\n",
    "y = y[shuffled_index]\n",
    "gel1 = gel1[shuffled_index]\n",
    "\n",
    "# Split\n",
    "split_index = int(len(gel)*0.8)\n",
    "train_gel, test_gel = gel[:split_index], gel[split_index:]\n",
    "train_cap, test_cap = cap[:split_index], cap[split_index:]\n",
    "train_att, test_att = att[:split_index], att[split_index:]\n",
    "train_y, test_y = y[:split_index], y[split_index:]\n",
    "train_gel1, test_gel1 = gel1[:split_index], gel1[split_index:]\n",
    "\n",
    "# make instances of the CNN\n",
    "mlp = create_mlp(train_att.shape[1], regress=False)\n",
    "gel_cnn = create_cnn(GEL_WIDTH, GEL_HEIGHT, 1, regress=False)\n",
    "cap_cnn = create_cnn(CAP_WIDTH, CAP_HEIGHT, 1, regress=False)\n",
    "gel1_cnn = create_cnn(CAP_WIDTH, CAP_HEIGHT, 1, regress=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## data 분포"
   ],
   "metadata": {
    "id": "sa9R_clC3RQz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.hist(y, bins=22)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "id": "vGJQGjh9uFzp",
    "outputId": "4d752117-3c98-41a4-edb9-6fa1b250a32e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 Result\n",
    "1. gel 이미지\n",
    "2. cap 이미지\n",
    "3. gel 이미지 + 속성\n",
    "4. cap 이미지 + 속성\n",
    "5. gel 이미지 + cap 이미지 + 속성\n",
    "\n",
    "include CM, ROC, AUC for all\n",
    "\n",
    "## 1 GEL"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########## Train ##########\n",
    "log_dir_1 = \"logs/fit/1_gel_\" + datetime.datetime.now().strftime(\"%m%d_%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_1, histogram_freq=1)\n",
    "es = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "\n",
    "gel_input = gel_cnn.outp\n",
    "x = Dense(4, activation=\"relu\")(gel_input)\n",
    "output = Dense(8, activation=\"softmax\")(x)\n",
    "\n",
    "model_1 = Model(inputs=gel_cnn.input, outputs=output)\n",
    "\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model_1.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=['acc'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_1.fit(train_gel, train_y, validation_split=0.1, epochs=16, batch_size=4, callbacks=[tensorboard_callback, es])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########## Evaluate ##########\n",
    "model_1.evaluate(test_gel, test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########## CM ##########\n",
    "y1 = model_1.predict(test_gel)\n",
    "predicted1 = np.argmax(y1, axis=1)\n",
    "\n",
    "draw_CM(test_y, predicted1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########## ROC, AUC ##########\n",
    "y1 = label_binarize(test_y, classes=classes)\n",
    "x1 = label_binarize(predicted1, classes=classes)\n",
    "\n",
    "draw_ROC_AUC(x1, y1, category_names)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 CAP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########## Train ##########\n",
    "log_dir_2 = \"logs/fit/2_CAP_\" + datetime.datetime.now().strftime(\"%m%d_%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_2, histogram_freq=1)\n",
    "es = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "\n",
    "cap_input = cap_cnn.output\n",
    "x = Dense(4, activation=\"relu\")(cap_input)\n",
    "output = Dense(8, activation=\"softmax\")(x)\n",
    "\n",
    "model_2 = Model(inputs=cap_cnn.input, outputs=output)\n",
    "\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model_2.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=['acc'])\n",
    "\n",
    "model_2.fit(train_cap, train_y, validation_split=0.1, epochs=16, batch_size=4, callbacks=[tensorboard_callback, es])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########## Evaluate ##########\n",
    "model_2.evaluate(test_cap, test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########## CM ##########\n",
    "y2 = model_2.predict(test_cap)\n",
    "predicted2 = np.argmax(y2, axis=1)\n",
    "\n",
    "draw_CM(test_y, predicted2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########## ROC, AUC ##########\n",
    "y2 = label_binarize(test_y, classes=classes)\n",
    "x2 = label_binarize(predicted2, classes=classes)\n",
    "\n",
    "draw_ROC_AUC(x2, y2, category_names)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 GEL + Attributes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "log_dir_3 = \"logs/fit/3_GEL+Att_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_3, histogram_freq=1)\n",
    "es = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "\n",
    "Input_3 = concatenate([mlp.output, gel_cnn.output])\n",
    "x = Dense(4, activation=\"relu\")(Input_3)\n",
    "output = Dense(len(category_names), activation=\"softmax\")(x)\n",
    "\n",
    "model_3 = Model(inputs=[mlp.input, gel_cnn.input], outputs=output)\n",
    "optimizer = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model_3.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "model_3.fit([train_att, train_gel], train_y, validation_split=0.1, epochs=16, batch_size=4, callbacks=[tensorboard_callback, es])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########## 평가 ##########\n",
    "model_3.evaluate([test_att, test_gel], test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########## CM ##########\n",
    "y3 = model_3.predict([test_att, test_gel])\n",
    "predicted3 = np.argmax(y3, axis=1)\n",
    "\n",
    "draw_CM(test_y, predicted3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########## ROC, AUC ##########\n",
    "y3 = label_binarize(test_y, classes=classes)\n",
    "x3= label_binarize(predicted3, classes=classes)\n",
    "\n",
    "draw_ROC_AUC(x3, y3, category_names)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 CAP + Attributes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "log_dir_4 = \"logs/fit/4_CAP+Att_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_4, histogram_freq=1)\n",
    "es = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "\n",
    "Input_4 = concatenate([mlp.output, cap_cnn.output])\n",
    "x = Dense(4, activation=\"relu\")(Input_4)\n",
    "output = Dense(len(category_names), activation=\"softmax\")(x)\n",
    "\n",
    "model_4 = Model(inputs=[mlp.input, cap_cnn.input], outputs=output)\n",
    "optimizer = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model_4.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=['acc'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_4.fit([train_att, train_cap], train_y, validation_split=0.1, epochs=16, batch_size=4, callbacks=[tensorboard_callback, es])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########## 평가 ##########\n",
    "model_4.evaluate([test_att, test_cap], test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########## CM ##########\n",
    "y4 = model_4.predict([test_att, test_cap])\n",
    "predicted4 = np.argmax(y4, axis=1)\n",
    "\n",
    "draw_CM(test_y, predicted4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########## ROC, AUC ##########\n",
    "y4 = label_binarize(test_y, classes=classes)\n",
    "x4 = label_binarize(predicted4, classes=classes)\n",
    "\n",
    "draw_ROC_AUC(x4, y4, category_names)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5 GEL + CAP + Att"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "log_dir_5 = \"logs/fit/5_GEL+CAP+Att_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_5, histogram_freq=1)\n",
    "es = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "\n",
    "Input_5 = concatenate([model_4.output, gel1_cnn.output])\n",
    "x = Dense(4, activation=\"relu\")(Input_5)\n",
    "output = Dense(len(category_names), activation=\"softmax\")(x)\n",
    "\n",
    "model_5 = Model(inputs=[model_4.input, gel1_cnn.input], outputs=output)\n",
    "optimizer = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model_5.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "model_5.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_5.fit([train_att, train_cap, train_gel1], train_y, validation_split=0.1, epochs=16, batch_size=4, callbacks=[tensorboard_callback, es])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########## 평가 ##########\n",
    "model_5.evaluate([test_att, test_cap, test_gel1], test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########## CM ##########\n",
    "y5 = model_5.predict([test_att, test_cap, test_gel1])\n",
    "predicted5 = np.argmax(y5, axis=1)\n",
    "\n",
    "draw_CM(test_y, predicted5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########## ROC, AUC ##########\n",
    "y5 = label_binarize(test_y, classes=classes)\n",
    "x5 = label_binarize(predicted5, classes=classes)\n",
    "\n",
    "draw_ROC_AUC(x5, y5, category_names)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 마무리"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########## localhost:6006 에서 tensorboad 접속 ##########\n",
    "%tensorboard --logdir logs/fit --host localhost --port 6006"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}